{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "from datasets import load_from_disk\n",
    "from evaluate import load\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from models.finetune import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # get all the articles, prepend each with \"bias;\"\n",
    "    inputs = [\n",
    "        f\"{bias}; {article}\"\n",
    "        for bias, article in zip(examples[\"summary_bias\"], examples[\"article\"])\n",
    "    ]\n",
    "    # tokenize the inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # tokenize the summaries, DO NOT TRUNCATE (unlike training)\n",
    "    targets = tokenizer(\n",
    "        examples[\"summary\"],\n",
    "        max_length=None,\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "    )\n",
    "\n",
    "    # set labels\n",
    "    model_inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    # return the tokenized data\n",
    "    # input_ids, attention_mask and labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 1024\n",
    "model_checkpoint = \"models/BART-SFT-r1/checkpoint-4200/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "metric = load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"early_stopping\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 142,\n",
       "  \"min_length\": 56,\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"num_beams\": 4,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"transformers_version\": \"4.29.2\"\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_checkpoint)\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['summary', 'article', 'article_bias', 'id', 'summary_bias'],\n",
       "        num_rows: 4664\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['summary', 'article', 'article_bias', 'id', 'summary_bias'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['summary', 'article', 'article_bias', 'id', 'summary_bias'],\n",
       "        num_rows: 602\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_from_disk(\"data/hf_dataset\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc708d538e9475b8b176c4aa4e85924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae69070def3c4597a16f4c88ea58160a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6991d6f55d4e569805eacb988d1647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/602 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = raw_datasets.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no training actually done here; just use Trainer as a wrapper for predict()\n",
    "batch_size = 4\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, article, summary_bias, article_bias. If summary, id, article, summary_bias, article_bias are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 602\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = trainer.predict(tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes around 15 min; let's load the test outputs directly\n",
    "import pickle\n",
    "with open(\"models/model_outputs/test.pkl\", 'rb') as f:\n",
    "    test_outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.259944438934326,\n",
       " 'test_rouge1': 42.931,\n",
       " 'test_rouge2': 15.2797,\n",
       " 'test_rougeL': 25.4485,\n",
       " 'test_rougeLsum': 37.9946,\n",
       " 'test_gen_len': 138.211,\n",
       " 'test_runtime': 834.4269,\n",
       " 'test_samples_per_second': 0.721,\n",
       " 'test_steps_per_second': 0.181}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_texts = [tokenizer.decode(token_ids, skip_special_tokens=True) for token_ids in test_outputs.predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bias left)\n",
      "Summary: The US House of Representatives has passed a bill that aims to overhaul the election and campaign finance systems in the US. The bill, known as the \"For The People Act\", passed 234-193 along party lines and aims to make voting more accessible and reduce corruption in the political process. It includes provisions to require automatic voter registration, make Election Day a federal holiday, and establish independent commissions to draw congressional districts to end partisan gerrymandering. The legislation also proposes that the sitting president and vice president, as well as candidates for the presidency and vice presidency, release their tax returns. However, Senate Majority Leader Mitch McConnell has stated that he will not give the bill a vote in his\n",
      "\n",
      "(bias right)\n",
      "Summary: The US House of Representatives has passed a bill that aims to overhaul the election and campaign finance systems in the US. The bill, known as the \"For The People Act\", was passed 234-193 along party lines. It includes provisions to require automatic voter registration, make Election Day a federal holiday, and introduce independent redistricting commissions to draw congressional districts. The legislation also requires the president and vice president to release their tax returns and nonprofits to disclose their large donors, taking aim at the \"dark money\" funding some political campaigns. However, Senate Majority Leader Mitch McConnell has stated he will not give the bill a vote in his chamber, effectively killing it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's look at left/right biased example outputs with id=3711, and article_bias=center\n",
    "indexes = []\n",
    "for index, item in enumerate(raw_datasets['test']):\n",
    "    if item['id'] == '3711' and item['article_bias'] == 'center' and item['summary_bias'] in ('left', 'right'):\n",
    "        indexes.append(index)\n",
    "\n",
    "for i in indexes:\n",
    "    print(f\"(bias {raw_datasets['test'][i]['summary_bias']})\")\n",
    "    print(f\"Summary: {predicted_texts[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(bias left)\n",
    "Summary: The US House of Representatives has passed a bill that aims to overhaul the election and campaign finance systems in the US. The bill, known as the \"For The People Act\", passed 234-193 along party lines and aims to <u>make voting more accessible and reduce corruption in the political process</u>. It includes provisions to require automatic voter registration, make Election Day a federal holiday, and establish independent commissions to draw congressional districts to <u>end partisan gerrymandering</u>. The legislation also proposes that the sitting president and vice president, as well as candidates for the presidency and vice presidency, release their tax returns. However, Senate Majority Leader Mitch McConnell has stated that he will not give the bill a vote in his\n",
    "\n",
    "(bias right)\n",
    "Summary: The US House of Representatives has passed a bill that aims to overhaul the election and campaign finance systems in the US. The bill, known as the \"For The People Act\", was passed 234-193 along party lines. It includes provisions to require automatic voter registration, make Election Day a federal holiday, and introduce independent redistricting commissions to draw congressional districts. The legislation also <u>requires</u> the president and vice president to release their tax returns and nonprofits to disclose their large donors, <u>taking aim at the \"dark money\" funding some political campaigns</u>. However, Senate Majority Leader Mitch McConnell has stated he will not give the bill a vote in his chamber, <u>effectively killing it</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
