# InstructSum
Note: I do not provide any dataset, data pre/post-processed files, or model checkpoints in this repo due to their large size. 

## Data Downloading / Generation / Preprocessing
The ALLSIDES dataset with full articles was generously provided by Nayeon Lee through an email as a zip file. I preprocess the data in `data/data.ipynb`. To generate the summaries, I use `scripts/gen_summ.sh`. This script leverages `scripts/api_request_parallel_processor.py`, which I copy from https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py. `data/data.ipynb` also formats the data into a jsonl file for this script, and processes the output jsonl file. I compute data statistics in `data/stats.ipynb`.

## Training Baselines
No baselines were trained for this project. GPT-3.5 and GPT-4 model outputs on the test set are generated by `scripts/gpt-3.5-test.sh` and `scripts/gpt-4-test.sh`, respectively. Like above, these scripts leverage `scripts/api_request_parallel_processor.py`, and `data/data.ipynb` contains code that processes the input and output jsonl files.

## Training Experiments
BART is fine-tuned in `model/finetune.py`. The current code contains the final configurations used in the report. The comparisons dataset is generated in `rlhf/comparisons_dataset.ipynb`, and annotated by `scripts/gpt-4-annotate.sh` (process similar to other scripts). The annotator prompt is in `rlhf/annotator_prompt.txt`, which I copy from https://github.com/tatsu-lab/alpaca_farm/blob/main/src/alpaca_farm/auto_annotations/annotators/greedy_gpt4/chatml_b5_with_inputs.txt. I also provide the reward model training code in `rlhf/train_rm.py`, although it still has some bugs that need to be fixed.

## Evaluation
I evaluate the fine-tuned BART model on the test set in `model/test.ipynb`. I plot training/validation loss and rouge in `models/plot.ipynb`. The baselines are evaluated in `model/baselines.ipynb`. Both notebooks also perform evaluation on the filtered test set. `rlhf/comparisons_dataset.ipynb` also evaluates the quality of the model output pairs along with how different two outputs from the same pair are from each other.
